{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About UnifyBio","text":"<p>UnifyBio is a set of power tools for data-driven data harmonization, ETL, analysis, and visualization, designed for use by informaticians in the translational sciences. UnifyBio is aimed at building and co-locating high quality clinical and molecular datasets by extracting data out of ad hoc tables, schemas, and siloes, and into unified representations with a simple, declarative workflow. UnifyBio's distributed data formats store highly dimensional and relationally intertwined datasets efficiently in distributed systems, making them available for full query, semantically rich exploration, and efficient processing, whether by humans or automated algorithms. UnifyBio tools enable all of this without sacrificing either flexibility or reproducibility, using schema inference rather than hard-coded logic whenever feasible, and preserving granular and simple to inspect time provenance for all datasets and system interactions.</p>"},{"location":"#getting-started-with-unify","title":"Getting Started with Unify","text":"<p>The steps of the most straightforward UnifyBio workflow are:</p> <ul> <li>preprocess a dataset for import</li> <li>write an import config for your dataset</li> <li>use the unify cli to import the dataset into a UnifyBio local system</li> <li>query the dataset via Python or R libraries.</li> </ul> <p>The easiest way to get started using UnifyBio with this workflow is to stand up a local dev system, and work through the quickstart. Depending on your learning preferences, you may want to either read the Concepts documentation before starting and/or refer to it as you go.</p>"},{"location":"#support","title":"Support","text":"<p>UnifyBio builds off of the core tooling of the CANDEL platform, generously open sourced by the Parker Institute for Cancer Immunotherapy. As of Fall 2024, development has been supported by the Rare Cancer Research Foundation and Clojurists Together.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>The core of UnifyBio's functionality is built around Unify, a domain and schema agnostic tool for data-driven ETL, focused around harmonizing scientific data into a hybrid distributed data system, consisting of one or many Datomic databases and an object store such as Amazon s3.</p> <p>UnifyBio is a set of libraries, schemas, and various tools that make use of the Unify CLI and its schema annotations, as well as common conventions for biological data, in order to harmonize disparate data into common data models, store data in granular time pronvenaced storage, and to provide data consistency and quality checks. UnifyBio also provides an ecosystem of downstream access and analysis tools that enable integrations with and exports to a variety of systems, both fulfilling and going beyond FAIR Principles. </p>"},{"location":"concepts/#unify-cli","title":"unify CLI","text":"<p>The Unify CLI is an executable jar file, optionally released or deployed with additional configuration and data, with execution wrapped via shell scripts. Unify is open source and Apache licensed and available at this repository.</p> <p>By convention, UnifyBio projects such as the Pattern Data Commons distribute Unify as part of a software package/distribution in a <code>bin/</code> subdirectory, where it can be accessed via commands such as:</p> <pre><code>bin/unify --import-config ~/import-name/config.yaml --working-directory ~/prepared-data/import-name\n</code></pre>"},{"location":"concepts/#datomic","title":"Datomic","text":"<p>Unify uses Datomic as its central store of record, and stores all relational data in Datomic databases. Unify is designed to work with multiple Datomic databases, and most UnifyBio workflows expect multiple databases, possibly containing multiple versions of datasets, with a central database which indexes and tracks dataset states and readiness. Most of UnifyBio's affordances re: granular time provenance, sparse representations, and arbitrarily accessible data are architecture properties hoisted through its use of Datomic.</p>"},{"location":"concepts/#unify-schema","title":"Unify schema","text":"<p>A Unify schema as stored in Datomic consists of four different components:</p> <ul> <li>Unify's own schema, such as attribute definitions used by the metamodel, or   import metadata and attributes.</li> <li>Base Datomic schema, e.g. all the attributes used in a particular data model.</li> <li>The metamodel, a list of schema annotations about relations between entities   and which attributes encode different kinds of identity (more in metamodel section).</li> <li>The enum set: a list of all enum values used in the schema.</li> </ul> <p>For one example full schema, see the reference CANDEL schema in the Vendekagon Labs Unify repository.</p> <p>See the schema docs for more information about schemas and other example schemas.</p> <p>You can also visualize an example schema hosted by RCRF here.</p>"},{"location":"concepts/#the-unify-metamodel","title":"The Unify Metamodel","text":"<p>Unify's metamodel enables the Unify CLI's declarative, data-driven ETL process. The metamodel encodes relations between entities by annotating relevant attributes, e.g. that the <code>:dataset/assays</code> attribute points to <code>assay</code> entities, and that an <code>assay</code> kind is a child of the <code>dataset</code>, and has children <code>measurement sets</code>, which have children <code>measurements</code>, and so on.</p> <p>See the metamodel section of the schema docs for more details.</p>"},{"location":"concepts/#datasets","title":"Datasets","text":"<p>The Unify CLI expects that all data being batch imported is modeled within the framework of a dataset. I.e., that subjects, assays, samples, clinical observations, etc. are all part of a particular dataset. This does not necessarily constraint downstream tooling (e.g. a query can look at all subjects, not just all subjects for a dataset) but does structure imports and import config files. See the Import Config docs for more information.</p>"},{"location":"concepts/#reference-data","title":"Reference Data","text":"<p>The Unify data model specifies that some of the contents of the database are Reference Data. Reference data consists of standard identifiers, controlled vocabularies, ontologies, and so on. It also refers to common conventions that can be used to uniquely and globally identify things like the biological entities that measurements are intended to target, even when not exhausitvely modeled in the data, such as variants and genomic coordinates. Some reference data can be imported with the Unify CLI via special forms in the import config, more information about reference data can be found in the schema docs.</p>"},{"location":"concepts/#seed-data","title":"Seed Data","text":"<p>Seed data refers to the the portion of reference data which is bulk loaded into UnifyBio databases on creation. Due to CANDEL terminology priors, this is sometimes referred to as bootsrap data. This is usually data from a standards provider, e.g. genes and gene products from HGNC and proteins and epitopes from UniProt. Every UnifyBio distribution should provide seed data (or e.g. scripts for downloading seed data) as part of their software distribution.</p>"},{"location":"concepts/#import-config-files","title":"Import Config Files","text":"<p>An import config file specifies the mapping from the conents of several data files (e.g. the ad hoc schema implied by a TSV file's column names) into the particular UnifyBio system's schema. Import config files are specified in detail in the [import config][import-config.md] docs.</p>"},{"location":"concepts/#working-directory","title":"Working Directory","text":"<p>The working directory is a directory that the Unify CLI writes to and stores intermediate outputs and representations in while data is being prepared, and which contains all the contents needed to transact data.</p>"},{"location":"concepts/#prepare-task","title":"Prepare Task","text":"<p>Before data can be transacted into a Unify system's datomic database and object storage, the import config must be used to create edn data, which can be transacted into Datomic database systems (or, with future planned extensions, into other storages).</p> <p>Details on running prepare are documented in the Unify CLI docs.</p>"},{"location":"concepts/#transact-task","title":"Transact Task","text":"<p>Transaction takes data off disk and stores it in the distributed representation model specified by a UnifyBio system. Once data has been prepared into a working directory, it is ready to be transacted.</p> <p>Details on running transact are documented in the Unify CLI docs.</p>"},{"location":"concepts/#inference-tasks","title":"Inference tasks","text":"<p>The Unify schema and metamodel are used not only for inference throughout the UnifyBio stack, but also for generating integrations with other tools and systems. This includes things like inferring a metaschema for the Trino integration via Datomic analytics, or inferring a JSON schema for enabling editors to provide static analysis and autocomplete functionality to users writing import configs.</p> <p>The available <code>infer</code> tasks are documented in the Unify CLI docs.</p>"},{"location":"concepts/#database-management","title":"Database Management","text":"<p>UnifyBio systems typically manage multiple Datomic databases. Some of this is done through the Unify CLI, requesting databases to be used for transacting imports and validating data. In general, mature UnifyBio systems have to implement some degree of database management and administration as part of dataset curation and historical data preservation for users.</p>"},{"location":"concepts/#datomic_1","title":"Datomic","text":"<p>UnifyBio uses Datomic as a central data store for all relational data and some measurement data. Datomic is a database with a sparse core representation (datoms) that provides granular time travel capabilities and strong transactional guarantees. See the official Datomic docs for more information.</p>"},{"location":"concepts/#datalog-query","title":"Datalog query","text":"<p>The core data representation in UnifyBio can be queried directly and efficiently using Datomic's dialect of datalog. Users should refer to Datomic's query reference material for full documentation on capabilities.</p>"},{"location":"concepts/#sql-integration","title":"SQL Integration","text":"<p>UnifyBio uses Datomic Analytics' Trino integration to map data from Datomic into tabular views that can be accessed over SQL through the appropriate Trino or PrestoDB adaptors. See the Trino and  Datomic Analytics docs for more information on how the integration works and how it can be used to integrate UnifyBio systems with traditional data warehouses and tools that map analysis workflows and visualization to generated SQL like Tableau or Apache Superset.</p>"},{"location":"concepts/#object-storage-and-highly-dimensional-molecular-data","title":"Object storage and highly dimensional molecular data","text":"<p>UnifyBio stores highly dimensional molecular data in measurement matrices, which are then linked via identifiers to the primary Datomic databases where relational assay and clinical data are stored. There are future plans for storing HDF, Zarr, Parquet, and similar formats directly to provide distributed access to external analysis ecosystems like Seurat and scverse.</p>"},{"location":"import-config/","title":"Import Config","text":"<p>An import config file specifies a mapping from ad hoc tables and their contents to a UnifyBio schema's data model.</p>"},{"location":"import-config/#writing-import-config-files","title":"Writing import config files","text":"<p>The template dataset showcases most of the functionality of UnifyBio. The config file is available in both yaml and edn forms. If you're new to UnifyBio, we recommend you start with yaml since Unify is able to generate JSON schema that editors can use to provide autocompletion and static analysis for import config files.</p>"},{"location":"import-config/#configuring-json-schema-for-editor-assistance-with-yaml-config-files","title":"Configuring JSON Schema for Editor Assistance with YAML config files","text":"<p>If you are using the Pattern distribution, <code>pattern-import-config-schema.json</code> has been pre-generated and provided for you. If you are not, you can generate a JSON schema for your own Unify schema with:</p> <pre><code>bin/unify infer-json-schema --json-schema YOUR-FILE-NAME.json\n</code></pre> <p>Most editors support applying json schema to YAML files as you edit, we provide links for the docs and plugins on how to do so below:</p> <ul> <li>VSCode<ul> <li>RedHat YAML extension</li> <li>Go to settings (Code &gt; Settings &gt; Settings on Mac) and add a path to the JSON schema file under the key \"yaml.schemas\".<ul> <li>More detailed instructions here.</li> </ul> </li> </ul> </li> <li>JetBrains products (e.g. IntelliJ, PyCharm, etc.)<ul> <li>Method 1: provide configuration via the JSON Schema Mappings setting.</li> <li>Method 2: annotate the YAML file with a path to the spec in a comment.</li> </ul> </li> </ul>"},{"location":"import-config/#general-config-file-structure","title":"General config file structure","text":"<p>The nesting of the different objects in the config file mirrors the relationship between the corresponding entities in the database. The top level import section, for instance, contains information about the data import. Keys that begin with <code>unify/</code> (i.e., that are in the unify namespace) usually specify metadata about the import or directives (special commands or syntax) for performing transforms that require more than parsing a data literal in a TSV file into an expected type.</p> <p>Data that is not nested under the <code>dataset</code> entity is reference data, and refers to data from ontologies or standards, or other cases for which identifiers should always be valid across datasets. The following examples conform to the Pattern schema.</p> yaml import config structure<pre><code>unify/import:\ndataset:\n  assays:\n    - measurement-sets:\n      - measurements:\n  samples:\n  subjects:\n  clinical-observations:\n  treatment-regiments:\n  timepoints:\ngenomic-coordinate:\nvariant:\ncnv:\n</code></pre> edn import config structure<pre><code>{:unify/import \n :dataset\n {:assays\n  [:measurement-sets\n   [:measurements]]}\n  :samples []\n  :subjects []\n  :clinical-observations []\n  :treatment-regimens []\n  :timepoints []}\n :genomic-coordinate []\n :variant []\n :cnv []}\n</code></pre>"},{"location":"import-config/#data-specifications","title":"Data specifications","text":"<p>Now let's look into specific section to understand how data mapping with unify works. </p>"},{"location":"import-config/#literal-data","title":"Literal data","text":"<p>The data below is literal data; it will be input into the database as is. The name of the dataset will be <code>template-dataset</code>, the doi will be <code>10.1126/science.aaa1348</code> etc. This is as opposed to data that is input from a file -- see below for many examples of this.</p> yaml dataset literal<pre><code>dataset:\n  name: template-dataset\n  description: \"Description of your dataset, ie title of paper, title of clinical trial, or description of conglomerate dataset\"\n  doi: \"10.1126/science.aaa1348\"\n  url: \"http://science.sciencemag.org/content/suppl/2015/03/11/science.aaa1348.DC1\"\n</code></pre> edn dataset literal<pre><code>:dataset {:name         \"template-dataset\"\n          :description  \"Description of your dataset, ie title of paper, title of clinical trial, or description of conglomerate dataset\"\n          :doi          \"10.1126/science.aaa1348\"\n          :url          \"http://science.sciencemag.org/content/suppl/2015/03/11/science.aaa1348.DC1\"}\n</code></pre>"},{"location":"import-config/#loading-data-from-input-tsv-files","title":"Loading data from input TSV files","text":"<p>To load data from input files (which must be in TSV format), we use the <code>:unify/input-tsv-file</code> key (see below). Note that the input file path is relative to the <code>config.edn</code> or <code>config.yaml</code> file location.</p>"},{"location":"import-config/#static-attributes-standard-table-wide-format","title":"Static attributes (standard table / wide format)","text":"<p>This is the simplest type of data. Each column correspond to an attribute and each row to an entity. The mappings between columns in the original data and attributes in Unify are specified as follows:</p> yaml measurements file directive<pre><code>measurements:\n  - unify/input-tsv-file: 'processed/nanostring.txt'\n    gene-product: variable\n    sample: sample\n    nanosring-count: value\n</code></pre> edn measurements file directive<pre><code>:measurements [{:unify/input-tsv-file  \"processed/nanostring.txt\"\n                :gene-product     \"variable\"\n                :sample           \"sample\"\n                :nanostring-count \"value\"}]\n</code></pre> <p>This is what the corresponding <code>nanostring.txt</code> file looks like:</p> sample variable value TCGA-06-5416-01A-01D-1486-08 ACO2 9.795 TCGA-06-5416-01A-01D-1486-08 ACTB 15.25 TCGA-06-5416-01A-01D-1486-08 ACVR1C 5.603 TCGA-06-5416-01A-01D-1486-08 ACVR2A 7.545 TCGA-06-5416-01A-01D-1486-08 ACVR2B 6.173 TCGA-06-5416-01A-01D-1486-08 ADA 6.532"},{"location":"import-config/#dynamic-attributes-molten-format","title":"Dynamic attributes (molten format)","text":"<p>When data is in a melted or similar sparse format there will be a column in the output file that specifies the attribute type of each row, and another column with the actual value (i.e. similar to the output of the <code>melt</code> function in the <code>reshape</code> R package or  melt in the Python pandas package.). Molten data can be imported using the special form below. Note that as is often the case, this contains a mixture of wide and molten data.</p> <p>The molten import syntax is demonstrated in the highlighted code block examples below. The column specified by <code>unify/variable</code> contains the attribute name, and the column specified by <code>unify/value</code> indicates which column contains the value that corresponds to said attribute. The mapping in <code>unify/variables</code> specifies how each variable name that appears in the file maps into the Unify schema. E.g., the variable <code>t.depth</code> corresponds to the measurement attribute <code>:measurement/t-depth</code>.</p> yaml dynamic attributes<pre><code>measurements:\n  - unify/input-tsv-file: \"processed/variant_measurements.txt\"\n    sample: \"sample.id\"\n    variant: \"var.id\"\n    unify/variable: \"variable\"\n    unify/value: \"value\"\n    unify/variables:\n      t.depth: \":measurement/t-depth\"\n      n.depth: \":measurement/n-depth\"\n      vaf: \":measurement/vaf\"\n</code></pre> edn dynamic attributes<pre><code>:measurements [{:unify/input-tsv-file \"processed/variant_measurements.txt\"\n                :sample               \"sample.id\"\n                :variant              \"var.id\"\n                :unify/variable       \"variable\"\n                :unify/value          \"value\"\n                :unify/variables      {\"t.depth\" :measurement/t-depth\n                                       \"n.depth\" :measurement/n-depth\n                                       \"vaf\"     :measurement/vaf}}]}\n</code></pre> <p>For reference, this is what the corresponding <code>variant_measurements.txt</code> file looks like</p> sample.id var.id variable value AL4602_T GRCh38:chr1:+:27023633:27023633/G/A n.depth 11 AL4602_T GRCh38:chr1:+:159898129:159898129/G/A n.depth 32 AL4602_T GRCh38:chr1:+:237024557:237024557/G/T n.depth 158 MA7027_T GRCh38:chr9:+:101797386:101797386/G/A t.depth 144 MA7027_T GRCh38:chr9:+:130213074:130213074/C/A t.depth 10 MA7027_T GRCh38:chr9:+:131587312:131587312/T/A t.depth 9 FR9547_T GRCh38:chr2:+:103090441:103090441/C/A vaf 0.32 FR9547_T GRCh38:chr2:+:172967053:172967053/G/T vaf 0.23 FR9547_T GRCh38:chr2:+:207176120:207176120/C/T vaf 0.3"},{"location":"import-config/#processing-multiple-files-at-once","title":"Processing multiple files at once","text":"<p>You can use a series of files as input if you specify them as a glob. The example below would process each file matching the regular expression</p> <pre><code>measurements:\n  - input-tsv-file:\n    unify.glob/directory: \"processed/\"\n    unify.glob/pattern: \"variant_meas_*.tsv\"\n</code></pre> <pre><code>measurements [{:unify/input-tsv-file #glob[\"processed/\" \"variant_meas_*.tsv\"]}]\n</code></pre>"},{"location":"import-config/#dealing-with-missing-values","title":"Dealing with missing values","text":"<p>The <code>:unify/na</code> key tells unify what to treat as NA in the input file. Attributes containing a value specified as <code>na</code> will be ignored and not input into the database. The <code>:unify/omit-if-na</code> lists a set of attributes that will cause the entire entity to be omitted if any of these attributes is NA. This is usually applicable to measurements.</p> yaml NA example<pre><code>measurements:\n  - unify/input-tsv-file: \"processed/variant_meas_1.tsv\"\n    unify/na:\n    - \"\"\n    - \"NA\"\n    unify/omit-if-na:\n      - \":measurement/t-ref-count\"\n      - \":measurement/t-alt-count\"\n    sample: Tumor_Sample_Barcode\n    variant: var.id\n    unify/variable: variable\n    unify/value: value\n    unify/variables:\n      t_ref_count: \":measurement/t-ref-count\"\n      t_alt_count: \":measurement/t-alt-count\"\n</code></pre> edn NA example<pre><code>:measurements [{:unify/input-tsv-file \"processed/variant_meas_1.tsv\"\n                :unify/na         \"\"\n                :unify/omit-if-na [:measurement/t-ref-count\n                                  :measurement/t-alt-count]\n                :sample          \"Tumor_Sample_Barcode\"\n                :variant          \"var.id\"\n                :unify/variable    \"variable\"\n                :unify/value       \"value\"\n                :unify/variables  {\"t_ref_count\" :measurement/t-ref-count\n                                   \"t_alt_count\" :measurement/t-alt-count}}]\n</code></pre> <p>Below is what the corresponding data files looks like. </p> <p>As you can see the 4th row is missing a value. Becuase of the usage of <code>:unify/omit-if-na</code> the entity generated by this row will be discarded. If you had not used <code>:unify/omit-if-na</code> this would have generated a measurement entity with <code>:measuremnent/sample</code> and <code>:measurement/variant</code> attributes, but no value. This is an invalid entity and would have failed validation</p> Tumor_Sample_Barcode var.id variable value TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr1:+:19232629:19232629/C/T t_ref_count 77 TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr1:+:91708723:91708723/G/A t_ref_count 45 TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr2:+:46380261:46380261/C/T t_ref_count 81 TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr1:+:19232629:19232629/C/T t_alt_count TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr1:+:91708723:91708723/G/A t_alt_count 11 TCGA-WB-A814-01A-11D-A35I-08 GRCh38:chr2:+:46380261:46380261/C/T t_alt_count 37"},{"location":"import-config/#mapping-of-categorical-variables","title":"Mapping of categorical variables","text":"<p>Most categorical attributes are represented in the database as enums as opposed to string (e.g. sex is represented by the enums <code>:sex/female</code> and <code>:sex/male</code>). The <code>mappings.edn</code> or <code>mappings.yaml</code> file (specified in the config in <code>[:unify/import :mappings]</code>) dictates how unify will map string values from TSV files in your data to enums. The following snippet is an example from the template dataset:</p> <p>The <code>:unify/mappings</code> section of the file defines a set of mappings. The left side is the name of the mapping and these names can be re-used in mappings files. The right side defines the mapping, where the keys are valid data literals or enums in the schema, and the right is a vector (or array or list) of string values that remap to the key. The <code>:unify/variables</code> top level key in the file specifies which attributes (keys) each of the provided mappings (values) will apply to.</p> yaml mappings example<pre><code>unify/mappings:\n  enum/metastasis:\n    true:\n      - \"Met\"\n      - \"T\"\n    false:\n      - \"Primary\"\n      - \"N/A\"\n      - \"F\"\n      - \"\"\n  enum/sex:\n    sex/female:\n    - \"F\"\n    sex/male:\n    - \"M\"\nunify/variables:\n  sample/metastasis: \"enum/metastasis\"\n  subject/sex: \"enum/sex\"\n</code></pre> edn mappings example<pre><code>{:unify/mappings {:enum/metastasis {true [\"Met\" \"T\"]\n                                    false [\"Primary\" \"N/A\" \"F\" \"\"]}\n                  :enum/sex {:sex/female [\"F\"]\n                             :sex/male [\"M\"]}}\n\n :unify/variables {:sample/metastasis :enum/metastasis\n                   :subject/sex :enum/sex}}\n</code></pre>"},{"location":"import-config/#reverse-references","title":"Reverse references","text":"<p>In some cases you may have to specify a reference between two entities <code>A -&gt; B</code> but the entity type B does not have a unique identifier while A does. The only part of the schema where this currently happens is when you are specifying therapies for subjects. Subjects refer to therapies but, while subjects have an id, therapies do not. To handle such cases unify provides the option to specify references in reverse.</p> <p><code>:unify/rev-variable</code> specifies which column in the therapies file (<code>subject.id</code> in this case) refers back to the subject id in the subject file. <code>:unify/rev-attr</code> specifies which attribute in the parent entity (i.e. the subject) this reference should be attached to (<code>:subject/therapies</code> in this case).</p> yaml reverse ref example<pre><code>subjects:\n  - unify/input-tsv-file: \"processed/rizvi-subjects.txt\"\n    id: \"subject.id\"\n    sex: gender\n    meddra-disease: disease\n    smoker: smoker\n    therapies:\n    - unify/input-tsv-file: \"processed/rizvi-therapies.txt\"\n      treatment-regimen: \"regimen\"\n      line: \"line\"\n      unify/reverse:\n        unify/rev-variable: \"subject.id\"\n        unify/rev-attr: \":subject/therapies\"\n</code></pre> edn reverse ref example<pre><code>:subjects [{:unify/input-tsv-file \"processed/rizvi-subjects.txt\"\n            :id              \"subject.id\"\n            :sex             \"gender\"   \n            :meddra-disease  \"disease\"\n            :smoker          \"smoker\"\n            :therapies       {:unify/input-tsv-file   \"processed/rizvi-therapies.txt\"\n                              :treatment-regimen \"regimen\"\n                              :line              \"line\"\n                              :unify/reverse      {:unify/rev-variable \"subject.id\"\n                                                   :unify/rev-attr     :subject/therapies}}}\n</code></pre> <p>This is what the subjects file looks like:</p> subject.id disease gender smoker AL4602 Lung adenocarcinoma M Former AU5884 Lung adenocarcinoma M Former BL3403 Lung adenocarcinoma F Former CA9903 Lung adenocarcinoma M Former <p>and this is the therapies file</p> subject.id line regimen AL4602 1 Pembrolizumab-3wk AU5884 3 Pembrolizumab-2wk BL3403 2 Pembrolizumab-2wk CA9903 4 Pembrolizumab-3wk"},{"location":"import-config/#data-fields-with-multiple-values","title":"Data fields with multiple values","text":"<p>If your data has multiple values for an attribute in a single cell of the input table, you can tell unify to split the content of the cell into individual values. For instance in the following example we have a single <code>Genes</code> column that contains multiple genes separated by <code>;</code>. <code>:cnv/genes</code> is a cardinality-many attribute, and therefore the set of genes in each cell will all be associated with a single entity.</p> <p>This snippet shows how to direct Unify to handle these cases:</p> yaml multiple value example<pre><code>cnv:\n  - unify/input-tsv-file: \"processed/cnv_ref_3.tsv\"\n    genomic-coordinates: \"gc.id\"\n    id: \"gc.id\"\n    genes:\n      unify/many-delimiter: \";\"\n      unify/many-variable: \"Genes\"\n</code></pre> edn multiple value example<pre><code>:cnv {:unify/input-tsv-file \"processed/cnv_ref_3.tsv\"]\n      :genomic-coordinates \"gc.id\"\n      :id  \"gc.id\"\n      :genes {:unif/many-delimiter \";\"\n              :unif/many-variable \"Genes\"}}\n</code></pre> <p>This is what the corresponding data table looks like</p> gc.id Genes GRCh38:1:+:74450881:74911796 CRYZ;ERICH3;ERICH3-AS1;TYW3 GRCh38:2:+:120251789:120441226 INHBB;RALB GRCh38:2:+:125852095:126987310 GYPC;LINC01941;TEX51"},{"location":"import-config/#constant-values","title":"Constant values","text":"<p>You can use the <code>:unify/constants</code> directive to specify values that you want to be constant across all the entities in a file. For instance in the following example all the measurements entity generated by pret will have a <code>:measurement/epitope</code> attribute with value <code>LDHA</code>.</p> yaml constants example<pre><code>:measurements\n  - unify/input-tsv-file: \"processed/ldh_meas_and_samples.txt\"\n    sample: \"sample.id\"\n    measurement/ng-mL: \"LDH @ Baseline\"\n    unify/constants\n      - measurement/epitope: \"LDHA\"\n</code></pre> edn constants example<pre><code>:measurements {:unif/input-tsv-file   \"processed/ldh_meas_and_samples.txt\"\n               :sample                \"sample.id\"\n               :measurement/ng-mL     \"LDH @ Baseline\"\n               :unif/constants        {:measurement/epitope \"LDHA\"}}}]}\n</code></pre>"},{"location":"local-system/","title":"Local System Setup","text":"<p>UnifyBio supports use in a multitude of configurations, from individual laptops to on-premise systems in labs and organizations and large, distributed cloud systems. For both dev convenience, and to enable individual scientists and small teams, UnifyBio provides two local system configuration options.</p>"},{"location":"local-system/#local-system-management-with-docker-compose","title":"Local System Management with Docker Compose","text":"<p>By convention, software distrubtions that contain UnifyBio tools ship with a local docker compose system that consists of:</p> <ul> <li>a Postgres database, which acts as Datomic's underlying storage.</li> <li>a Datomic transactor</li> <li>a Query Service,   with a JSON Query API that backs the R and Python libraries</li> </ul> <p>Local systems write high dimensional measurement data to file storage instead of cloud object storage, so measurement matrices and HDF, parquet, and zarr files will be stored in a subdirectory and accessed from there (this configuration can be overwritten).</p> <p>To use the local docker compose system, you will need to install Docker Desktop or the Docker Engine, depending on your platform.</p> <p>After obtaining a UnifyBio software distribution, you can run:</p> <pre><code>bin/start-local-system\n</code></pre> <p>To start a local system. Note: while this script is typically provided as a convenience, all that is stricly necessary is to navigate the directory which contains the <code>docker-compose.yml</code> file and run <code>docker-compose up</code>.</p>"},{"location":"local-system/#local-system-management-through-jvm-process-management","title":"Local System Management through JVM process management","text":"<p>While containerization provides user convienence, it also results in operational complexity that is not stricly necessary for use. UnifyBio is tested and supports use with JVM 21+ conforming distributions of temurin, corretto, and Oracle Java. You can run any of the individual components of the local Unify system as components on the same machine, or distributed across several machines in any suitable network configuration.</p> <p>While UnifyBio's core tools are written in Clojure, they are distributed as uberjars and you will not need a Clojure distribution unless you intend to do development, in which case you should refer to Clojure's official installation and  getting started guides.</p>"},{"location":"other-resources/","title":"Other Resources","text":"<p>Because UnifyBio is built heavily around the use of Datomic and  Unify, users are encourages to refer to documentation and tutorials for these. We also recommend perusing resources related to CANDEL, the project which birthed prototypes of the tools used in UnifyBio.</p>"},{"location":"other-resources/#datomic","title":"Datomic","text":"<p>It's especially helpful to understand Datomic's datalog query model, even if you intend to primarily use UnifyBio via SQL integrations like that provided by the Trino integration, since the mapping from Datomic datoms to SQL results in some atypical data patterns (though depending on the particular system you're using, some of these peculiarities might be abstracted away via Views).</p> <p>Two excellent resources for coming to grips with Datomic datalog quickly are Learn Datalog Today and Max Datom. For a more thorough introduction, there are learning materials, slides, and video links available for the Day of Datomic series in the official Datomic documentation Day of Datomic topic.</p>"},{"location":"other-resources/#candel-resources","title":"CANDEL resources","text":"<p>In addition to the website and resources in the GitHub repos there have been multiple talks on CANDEL, linked here for convenience.</p>"},{"location":"other-resources/#building-a-unified-cancer-immunotherapy-data-library","title":"Building a Unified Cancer Immunotherapy Data Library","text":""},{"location":"other-resources/#clojure-where-it-counts-tidying-data-science-workflows","title":"Clojure Where it Counts: Tidying Data Science Workflows","text":""},{"location":"other-resources/#four-years-of-datomic-powered-etl-in-anger-with-candel","title":"Four Years of Datomic Powered ETL in Anger with CANDEL","text":""},{"location":"schema/","title":"Schema","text":"<p>UnifyBio's core data model consists of an extended and annotated Datomic schema. You should refer to the official Datomic docs to understand the available primitive data types, attribute organization and cardinalities, and how references between entities are handled.</p>"},{"location":"schema/#unifys-metamodel","title":"Unify's Metamodel","text":"<p>Unify expects and enforces the convention, common in most Datomic schemas in practice, that that related attributes are grouped by entity type, termed kinds, to disambiguate  semantic information encoded in the data model from the use of programming language type systems and their constraints. In this convention, the type information is encoded in the name of the attribute by its namespace. For instance, in the attribute <code>:measurement/fpkm</code>, the <code>kind</code> enformation is encoded in the namespace portion, <code>measurement</code>.</p>"},{"location":"schema/#references-attributes","title":"References Attributes","text":"<p>Datomic's data model enforces no constraints and grants no affordances to this convention out of the box, so the Unify metamodel adds annotations to the database schema which provide additional constraints and affordances around the representaiton and use of kinds in the data model. Specifically, it specifies that certain reference attributes point from one kind to another, that certain kinds are children of other kinds (and inversely, some kinds are therefore parents to others) in the dataset tree.</p>"},{"location":"schema/#identity","title":"Identity","text":"<p>The Unify metamodel also specifies different ways that entities may be uniquely identified. Either an identification is assumed to be globally unique at import time, whether through a single attribute or composite ID, or the entity's identity must be scoped to its context in the dataset. Unforunately, it is all too common that various public and proprietary datasets only supply weak identifiers, such as integer keys, or string composites of type and integer, possibly with small string codes, e.g. \"trial-bcc-subject-1\". These keys do not provide the strong uniqueness guarantees that keys like UUIDs do, so on import, Unify will prefix these with context, into a tuple of dataset and the context scope from the dataset tree, e.g. <code>[\"dataset-name\" \"trial-bcc-subject-1\"]</code> or, with measurements, when the tree context and synthetic composite IDs are combined. <code>[\"dataset-name\" \"rna-seq/processing-method-1/sample-id--subject-id--BRCA</code>]`</p>"},{"location":"unify-cli/","title":"The Unify CLI","text":"<p>Thorough documentation for using the Unify CLI in UnifyBio projects.</p>"},{"location":"tutorials/first-import/","title":"Your First Import","text":"<p>This tutorial takes you through the process of creating, then importing your own minimal UnifyBio dataset. Before attempting to these steps, make sure you have a local environment capable of completing all the steps in the quickstart.</p> <p>The import we walk through is the synthetic <code>example-import</code> included in the Pattern UnifyBio distribution. It should be fairly simple to adapt to other small datasets; we encourage you to work through the tutorial a second time with some of your own data, once you've completed it for the example case.</p> <p>At any point, feel free to refer to the import config docs for a more thorough explanation of any special unify forms or syntax you encounter during the tutorial.</p>"},{"location":"tutorials/first-import/#organizing-files","title":"Organizing Files","text":"<p>The following shows an example directory structure for our simple import, which corresponds to conventions used in the Pattern Data Commons system:</p> <pre><code>example-import\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 mappings.yaml\n\u2514\u2500\u2500 data\n\u00a0\u00a0 \u2514\u2500\u2500 raw\n\u00a0\u00a0     \u251c\u2500\u2500 clinical.tsv\n\u00a0\u00a0     \u251c\u2500\u2500 measurements.tsv\n\u00a0\u00a0     \u251c\u2500\u2500 patients.tsv\n\u00a0\u00a0     \u2514\u2500\u2500 samples.tsv\n</code></pre> <p>For smaller and medium sized imports, Pattern uses ordinary git version control (sometimes with LFS enabled) to manage the state of the import that is the basis for the UnifyBio dataset. This ensures that database states can be linked back to git repos and commit hashes, guaranteeing a traceable history which can be used to source causes of error, or forward propagate changes if issues are found with the underlying dataset.</p> <p>You do not necessarily need to use this exact scheme in your own work, but some similar discipline about storing and tracking versions of data as it is imported into UnifyBio is recommended.</p> <p>Importantly, this captures the typical components of any import:</p> <ul> <li>the <code>config.yaml</code> and <code>mappings.yaml</code> files which contain the mappings and import directives which   specify to the Unify CLI how the data should be imported.</li> <li>raw data (stored in <code>data/raw</code>)</li> </ul> <p>You will also typically see:</p> <ul> <li><code>scripts/</code> for preprocessing the raw data into the form imported into the UnifyBio system.</li> <li>the data as transformed by the scripts, in <code>data/processed/</code></li> </ul>"},{"location":"tutorials/first-import/#using-tsvs","title":"Using TSVs","text":"<p>The present version of the Unify CLI can only process TSV files for input. Unify uses tab-separated files to avoid any of the ambiguities and problems that occur with arbitrarily delimited files, given that compact and obscure data encodings and fulltext natural language sentences and similar are often  imported and harmonized into clinical and molecular dataset. The TSV files do not need to correspond to any particular data model, but there should be a column to column mappings to attributes in the schema.</p>"},{"location":"tutorials/first-import/#creating-an-import-config","title":"Creating an Import Config","text":"<p>We will create our own import config file for the purposes of this tutorial. We will store it as a sibling to the current reference config file provided in the distribution and name it <code>tutorial-config.yaml</code>.</p> <pre><code>example-import\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 tutorial-config.yaml\n\u251c\u2500\u2500 mappings.yaml\n</code></pre> <p>To get started, provide the top level structure in the YAML file.</p> tutorial-config.yaml<pre><code>:unify/import:\n  user: \"some.user@gmail.com\"\ndataset:\n  name: \"my-first-import\"\n</code></pre>"},{"location":"tutorials/first-import/#setting-up-your-editor","title":"Setting Up Your Editor","text":"<p>The Unify CLI can generate a JSON Schema you can apply to your YAML editing process in order to provide static checks on the structure and content of the import, as well as to provide autocomplete, attribute documentation, and more. If you are using the Pattern UnifyBio distribution, this has been pregenerated for you and can be found under the distribution directry as the file <code>candel-import-config-schema.json</code>. If you need to generate it, you can do so with this CLI command:</p> <pre><code>bin/unify infer-json-schema --json-schema \"YOUR-FILE-NAME-HERE.json\"\n</code></pre> <p>Below are links for how to configure common editors to use JSON Schema when editing YAML files:</p> <ul> <li>VSCode, via the Red Hat YAML extension   steps are documented here.</li> <li>JetBrains products such as IntelliJ, Dataspell, and PyCharm, using the processes documented here.</li> </ul> <p>These are summarized in the Unify import config docs as well.</p>"},{"location":"tutorials/first-import/#providing-entries-for-each-file","title":"Providing Entries for each File","text":"<p>We have four files to provide mappings for:</p> <pre><code>patients.tsv\nclinical.tsv\nsamples.tsv\nmeasurements.tsv\n</code></pre> <p>In the following subsections you will write the import config for each of them.</p>"},{"location":"tutorials/first-import/#patient-subject-data","title":"Patient / Subject data","text":"<p>The <code>patients.tsv</code> file contains data on our patients. Patients are currently modeled under the <code>subject</code> entity in the Pattern schema. A list of available attributes can be found here. If you have configured your editor, you should get autocomplete suggestions as you type which contain the same documented information.</p> <p>In the file, we have the following columns and table shape:</p> id age sex disease subject-group-A-860 104 M lung adenocarcinoma subject-group-B-478 35 F lung adenocarcinoma subject-group-A-437 49 male lung adenocarcinoma <p>Looking at the patient data in the schema, we can find the appropriate attributes to map to:</p> <pre><code>id =&gt; :subject/id\nage =&gt; :subject/age\nsex =&gt; :subject/sex\ndisease =&gt; :subject/freetext-disease\n</code></pre> <p>(Well, the last one may not be obvious, but for the purpose of the tutorial, we'll have you use freetext instead of mapping the terms into an ontology.)</p> <p>In its simplest form, the yaml config file is just a literal translation of the mapping from columns to attributes that we just identified. The required update has been higlighted for you in the snippet below:</p> tutorial-config.yaml<pre><code>:unify/import:\n  user: \"some.user@gmail.com\"\ndataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n</code></pre> <p>There's actually one more thing we have to tend to here, though. <code>:subject/sex</code> values should be mapped to an enum, not simply read into the schema. You can find the enums in the schema visualizer.</p> <p>Mapping enums requires an additional <code>mappings.yaml</code> file. For now, we won't worry about writing it, but link to the existing <code>mappings.yaml</code> in the example import, then inspect the relevant contents.</p> tutorial-config.yaml<pre><code>:unify/import:\n  user: \"some.user@gmail.com\"\n  mappings: mappings.yaml\ndataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n</code></pre> <p>Here are the contents of mappings.yaml relevant to the <code>:subject/sex</code> attribute:</p> <p>mappings.yaml content<pre><code>unify/mappings:\n  enum/subject.sex:\n    subject.sex/male:\n      - \"Male\"\n      - \"M\"\n      - \"m\"\n      - \"male\"\n    subject.sex/female:\n      - \"Female\"\n      - \"female\"\n      - \"F\"\n      - \"f\"\nunify/variables:\n  subject/sex: enum/subject.sex\n</code></pre> This mapping lets Unify know that when it encounters literals like \"M\" or \"Male\" they should be mapped to <code>:subject.sex/male</code>, and likewise for \"F\" or \"female\" to <code>:subject.sex/female</code>.</p> <p>We are now ready to add more data! Note, that if you want to, you can incrementally run prepare or even transact as you go (assuming you're using local dev databases), just to check for errors or omitted mappings. For the purpose of the tutorial, we'll continue, but at any point you can refer back to the commonds from the quickstart and run the commands there on your import in progress.</p>"},{"location":"tutorials/first-import/#samples-data","title":"Samples data","text":"<p>Next we will map in <code>samples.tsv</code>. The columns and data layout are as below:</p> <pre><code>| anatomic-location | id               | patient-id          | timepoint |\n|-------------------|------------------|---------------------|-----------|\n| lung              | sample-HIKIKDGG2 | subject-group-A-860 | baseline  |\n| lung              | sample-JJGDJCEB1 | subject-group-B-478 | baseline  |\n| pancreas          | sample-BEFKJJCB7 | subject-group-A-437 | baseline  |\n| pancreas          | sample-IBHIBFDG3 | subject-group-B-62  | baseline  |\n</code></pre> <p>Again, we will consult the schema dashboard for samples to find the right attributes. The new mappings are provided below:</p> dataset portion of tutorial-config.yaml<pre><code>dataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n  samples:\n    - unify/input-tsv-file: \"data/raw/samples.tsv\"\n      id: \"id\"\n      subject: \"patient-id\"\n      freetext-anatomic-site: \"anatomic-location\"\n      timepoint: \"timepoint\"\n</code></pre> <p>There are two attributes used of a new kind here \u2014 timepoint and subject are both ref attributes. That means they refer to other entities, in this case, the <code>subject</code> and <code>timepoint</code> entities. You will notice, if you look through the tables, that we do not have a <code>timepoints.tsv</code> file. However, as you can quick verify with grep, R, pandas, or spreadsheet software, we only have two timepoints, the <code>baseline</code> timepoint and the <code>eos</code> timepoint.</p> <p>It is not necessary to add every data point in the dataset as a table, we can actually just use data literals in the import. </p>"},{"location":"tutorials/first-import/#adding-timepoints","title":"Adding timepoints","text":"<p>Data literal entities lack a <code>unify/input-tsv-file</code> directive. We'll add one for timepoints to our import config. First we'll consult the schema docs and see which attributes are available that we have the information to provide. It looks like we need to provide a <code>timepoint/id</code> and <code>timepoint/relative-order</code>. Since we only have two timepoints, we'll use <code>1</code> and <code>2</code> for the relative ordering. We can also click on the enum information for <code>timepoint/type</code> and we see our timepoint names <code>baseline</code> and <code>eos</code> map neatly on to two types, so we'll provide those values as well.</p> dataset portion of tutorial-config.yaml<pre><code>dataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n  samples:\n    - unify/input-tsv-file: \"data/raw/samples.tsv\"\n      id: \"id\"\n      subject: \"patient-id\"\n      freetext-anatomic-site: \"anatomic-location\"\n      timepoint: \"timepoint\"\n  timepoints:\n    - id: \"baseline\"\n      relative-order: 1\n      type: \":timepoint.type/baseline\"\n    - id: \"eos\"\n      relative-order: 2\n      type: \":timepoint.type/eos\"\n</code></pre> <p>Note: we supply the full enum attribute name as a string, e.g. <code>\":timepoint.type/baseline\"</code>. Unlike with attributes that are property names (i.e. map keys), Unify does not infer namespace information or keyword vs string type for literals that are property values. But it will attempt to parse any string in value position starting with <code>\":\"</code> as a keyword.</p>"},{"location":"tutorials/first-import/#mapping-in-measurementstsv","title":"Mapping in measurements.tsv","text":"<p>Measurements, as captured by the <code>measurement</code> entity/namespace, is a very broad entity type in the Pattern and CANDEL schemas. In general, any molecular assay or measurement taken in a lab is a measurement. When we refer to the measurement schema we can see a host of different measurement types available.</p> <p>You should think of measurements as duck typed, in that the kind of measurement represented is inferred from the attributes. Certain kinds of measurements have certain targets: proteomic assays target proteins, RNA-seq targets gene expression by way of transcript fragments, and so on. Most measurements are the result of an assay performed on a sample. UnifyBio's validator will ensure in the CLI <code>bin/validate</code> step that the combination of attributes on any measurement makes sense and corresponds to specs and expectations.</p> <p>In our case, we have a fairly straight forward RNA-seq assay to map in:</p> fpkm hugo sample 393.43107371469574 SNORA24B sample-HIKIKDGG2 203.42482862644985 TCAF1P1 sample-HIKIKDGG2 965.3775593576936 ARHGEF35-AS1 sample-HIKIKDGG2 320.0502226355575 PCDH19 sample-HIKIKDGG2 486.5600564050051 ANKRD49P1 sample-HIKIKDGG2 <p>It might not be obvious to non-biologists, but the <code>hugo</code> columns refers to genes (through the indirection of <code>gene-product</code> entities in the Pattern schema), and we can see <code>:measurement/fpkm</code> in the measurement entity in the schema, which stands for fragments per kilobase million, i.e. a normalized metric for RNA transcript counts.</p> dataset porition of tutorial-config.edn<pre><code>dataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n  samples:\n    - unify/input-tsv-file: \"data/raw/samples.tsv\"\n      id: \"id\"\n      subject: \"patient-id\"\n      freetext-anatomic-site: \"anatomic-location\"\n      timepoint: \"timepoint\"\n  timepoints:\n    - id: \"baseline\"\n      relative-order: 1\n      type: \":timepoint.type/baseline\"\n    - id: \"eos\"\n      relative-order: 2\n      type: \":timepoint.type/eos\"\n  assays:\n    - name: \"RNAseq\"\n      technology: \":assay.technology/RNA-seq\"\n      measurement-sets:\n        - name: \"standard fpkm\"\n          measurements:\n            - unify/input-tsv-file: \"data/raw/measurements.tsv\"\n              sample: \"sample\"\n              fpkm: \"fpkm\"\n              gene-product: \"hugo\"\n</code></pre> <p>A note on the tree structure: <code>:dataset/assays</code> contains all assays in a dataset, and for some datasets, like the one used in the Prince Study these can get quite large and detailed. Assays contain one or more measurement sets, which contain one or more measurement inputs, to provide groupings that might represent different experimental conditions, or different ways of processing data compuationally, etc. Sometimes these introduce derived entities, like cell populations grouped through flow cytometry, or single cells detected and separated through chemical or fluidic aspects of assays, or defined computationally through segmentation.</p> <p>Our case is pretty straightforward: we're referring to samples from elsewhere in our config, genes by names provided by the HGNC standard and already present as reference data.</p> <p>You'll note that as we've gone, also, we've built up the relational complexity of the dataset without doing a lot of work. For instance, each measurement is:</p> <ul> <li>a value or values representing an instrument reading in the real world, to which some   kind of data processing was applied.</li> <li>of a particular target, a known biological entity, which is a transcript (gene product) derived from a gene</li> <li>taken by a particular assay of a particular known technology type</li> <li>with certain processing and/or experimental conditions shared by other measurements   grouped into the same measurement set.</li> <li>of a sample that was derived<ul> <li>from a patient (subject)</li> <li>at a particular timepoint</li> </ul> </li> </ul> <p>Not every data model that uses UnifyBio's tool ecosystem or integrations, or every use of the Unify CLI for that matter, has to apply this sort of relational complexity. But this kind of relational complexity exists in many domains, and with minimal declarative effort, we are able to infer it from a combination of the structure of our data, and the details of our Unify schema and metamodel.</p>"},{"location":"tutorials/first-import/#clinicaltsv","title":"clinical.tsv","text":"<p>The last data we have to import is our clinical data. A view of it is below:</p> os patient timepoint 98 subject-group-A-860 eos 39 subject-group-B-478 eos 153 subject-group-A-437 eos <p>We have the overall survival data (as os), an ID for our patients, and a timepoint. In this case, all the ref targets (timepoints and patients) have been supplied, so the mapping is straight forward. Here it is in the context of the entire (and now complete) import:</p> complete tutorial-config.edn<pre><code>:unify/import:\n  user: \"some.user@gmail.com\"\n  mappings: \"mappings.yaml\"\ndataset:\n  name: \"my-first-import\"\n  subjects:\n    - unify/input-tsv-file: \"data/raw/patients.tsv\"\n      id: \"id\"\n      sex: \"sex\"\n      age: \"age\"\n  samples:\n    - unify/input-tsv-file: \"data/raw/samples.tsv\"\n      id: \"id\"\n      subject: \"patient-id\"\n      freetext-anatomic-site: \"anatomic-location\"\n      timepoint: \"timepoint\"\n  timepoints:\n    - id: \"baseline\"\n      relative-order: 1\n      type: \":timepoint.type/baseline\"\n    - id: \"eos\"\n      relative-order: 2\n      type: \":timepoint.type/eos\"\n  assays:\n    - name: \"RNAseq\"\n      technology: \":assay.technology/RNA-seq\"\n      measurement-sets:\n        - name: \"standard fpkm\"\n          measurements:\n            - unify/input-tsv-file: \"data/raw/measurements.tsv\"\n              sample: \"sample\"\n              fpkm: \"fpkm\"\n              gene-product: \"hugo\"\n  clinical-observation-sets:\n    - name: \"clinical observations\"\n      clinical-observations:\n        - unify/input-tsv-file: \"data/raw/clinical.tsv\"\n          subject: \"patient\"\n          os: \"os\"\n          timepoint: \"timepoint\"\n</code></pre> <p>You can refer to the clinical observations schema to see what other information can be encoded in clinical observations. As present, they play a similar role to measurements, being typed implicitly through the presence or lack of certain attributes. Entities can be grouped arbitrarily within clinical observation sets, representing different sites, contexts within a study or other medical interactions, labs, and so on.</p>"},{"location":"tutorials/first-import/#running-unify-cli-import-steps-with-your-dataset","title":"Running Unify CLI import steps with your dataset","text":"<p>You can now run all the same steps you ran in the quickstart to transact your data into a database. Because this is a new, unique dataset, you can transact it either into an existing dev database you already created, or a new one. You should probably make sure it passes validations first, though, before you consider making it available to others!</p> <p>The steps are:</p> <p><pre><code>bin/unify prepare --import-config PATH/TO/example-import/tutorial-config.yaml --working-directory TEMP-DIR/tutorial-attempt-1\nbin/unify request-db --database my-first-db\nbin/unify transact --working-directory TEMP-DIR/tutorial-attempt-1 --database my-first-db\nbin/unfy validate --database my-first-db\n</code></pre> Note: when you run validate without a <code>--dataset</code> arg, it defaults to the most recent dataset in the database.</p> <p>If your file contains typos or misaligned columns and attributes, you might encounter errors in a number of places:</p> <ul> <li>the config file may fail to parse with an error</li> <li>while data is being prepared, it might encounter an invalid data value or file vs. mapping situation,   e.g. a column specified in the config might not exist in the file.</li> <li>while transacting data, you might find that some values that are supposed to be unique   conflict with each other, or some reference data specified doesn't actually exist. E.g.   you might have a gene name that comes fom a different standard, or a different version of the HGNC standard even.</li> <li>while validating data, you might find that you created measurements with a nonsensical combination of   attributes, like a <code>:measurement/fpkm</code> attribute with a <code>:measurement/epitope</code> protein target. Or you   might find that you had a measurement that referred to sample a sample ID that didn't actually exist   in your sample table.</li> </ul> <p>Given these situations, it's completely normal and expected that you will hit hiccups and need to debug them. The Unify CLI tries to provide error messages that are as specific and direct as possible to make this as painless as possible. It can't magically provide the missing information from the bogus xslx file the vendor put in your Box folder, but it will do its best to identify any issues those sorts of things introduce.</p>"},{"location":"tutorials/first-import/#further-steps","title":"Further Steps","text":"<p>To see most of what's possible in a Unify import, refer to the template dataset and the import config docs. In addition, RCRF intends to make several curated imports and datasets public. To learn more, get in touch!.</p>"},{"location":"tutorials/quickstart/","title":"Quickstart","text":"<p>This section contains step by step instructions to import the template dataset, an example dataset constructed from multiple public data sources that exercises a large surface area of Unify's import functionality. Running these steps will help you make sure that everything is setup correctly on your system and also provide an overall view of the process</p>"},{"location":"tutorials/quickstart/#setup","title":"Setup","text":"<p>This quickstart assumes you have a local UnifyBio distribution downloaded, such as  the one provided by the Rare Cancer Research Foundation (RCRF)'s Pattern Data Commons. At present, UnifyBio is in an early alpha phase and only available in packaged form to RCRF's research partners. To get access to the Pattern distribution, you will need to contact RCRF.</p>"},{"location":"tutorials/quickstart/#local-system-via-docker-compose","title":"Local System via Docker Compose","text":"<p>To use the provided local system, you need to install Docker and ensure you can stand up a local system with Docker Compose. You will also need a JVM (version 21 or later required). You might already have one available, which you can check by running:</p> <pre><code>java -version\n</code></pre> <p>in a terminal. If you do not have a JVM installed, Temurin 21 or 22 is a good default.</p>"},{"location":"tutorials/quickstart/#additional-configuration","title":"Additional configuration","text":"<p>The current Pattern distribution requires hostname remapping at the OS level to ensure the Unify CLI's peer process can communicate with the other system components using the same configuration. Edit your <code>/etc/hosts</code> file and add these two entries:</p> /etc/hosts<pre><code>127.0.0.1   localhost\n127.0.0.1   transactor\n127.0.0.1   postgres\n</code></pre>"},{"location":"tutorials/quickstart/#starting-the-local-system","title":"Starting the local system","text":"<p>Download and unzip your UnifyBio distribution. If you have not been provided one, you can use the Pattern distribution provided by RCRF.</p> <ul> <li>Navigate to the directory created when you unzip or clone the UnifyBio distribution.</li> <li> <p>Start a local system with:</p> <pre><code>bin/start-local-system\n</code></pre> </li> </ul> <p>All subsequent commands assume you are in the UnifyBio distribution directory and have a local system running with Docker compose.</p>"},{"location":"tutorials/quickstart/#simple-import-workflow","title":"Simple Import Workflow","text":"<p>This quickstart shows an example of working with a minimal import, using the Unify CLI and the template dataset. The Pattern distribution contains a copy of the template-dataset in the <code>template-dataset/</code> subdirectory. You can also browse the copy used in Unify's test systems here.</p>"},{"location":"tutorials/quickstart/#prepare","title":"Prepare","text":"<p>The first thing you will do is transform data from tables into entity map representations in edn form, by using the Unify CLI prepare task.</p> <pre><code>bin/unify prepare --import-config $PATH-TO-TEMPLATE-DATASET/config.yaml --working-directory PATH-TO-TEMP-DIRECTORY\n</code></pre> <p>When prepare has completed successfully, you will be able to transact the data into a database.</p>"},{"location":"tutorials/quickstart/#request-database","title":"Request Database","text":"<p>Before we can transact data, we need a database to transact it into. For this quickstart, we'll request a dev database from the local UnifyBio system:</p> <pre><code>bin/unify request-db --database YOUR-DB-NAME\"\n</code></pre> <p>This will create a new Datomic database in your local system. In most UnifyBio workflows, you will work iteratively with dev databases to create, debug, and update the dataset import.</p> <p>Note: Local dev databases are suitable for learning Unify, but it is expected that production ready datasets will be published or otherwise blessed for use in a centralized system through an org specified process.</p>"},{"location":"tutorials/quickstart/#transact","title":"Transact","text":"<p>Now that we have prepared a dataset for import and have a database to import it into, we are ready to run <code>transact</code>. Note that <code>YOUR-DB-NAME</code> is the same <code>--database</code> you created in the request step.</p> <p><pre><code>bin/unify transact --database YOUR-DB-NAME --working-directory PATH-TO-TEMP-DIRECTORY\n</code></pre> The data is now in the database and can be queried, accessed through integrations, and validated.</p>"},{"location":"tutorials/quickstart/#validate","title":"Validate","text":"<p>UnifyBio distributions include a <code>validate</code> task, which ensures that data in a database conforms to org provided specifications. These ensure data are structurally and semantically sound, as well as referentially consistent (all targets of entity reference attributes refer to other entities that actually exist).</p> <p>Validate your import with this command:</p> <pre><code>bin/validate --database YOUR-DB-NAME --dataset \"template-dataset\"\n</code></pre>"},{"location":"tutorials/quickstart/#query-your-data","title":"Query Your Data","text":"<p>You can now run queries against the local database! If you are using the Pattern Data Commons distribution and schema, you can install the <code>patternq</code> Python or R libraries for free form query.</p> <p>The Pattern distribution contains an example query you can run with Python. This script accesses data through the Query service included in the local docker compose system. You will need to have the <code>requests</code> package in your local Python environment to run it.</p> <pre><code>python util/local-test-query.py YOUR-DB-NAME\n</code></pre> <p>You can optionally include a second arg to <code>local-test-query.py</code> that contains a file path to a JSON file containing a query request body. While covering the possible contents and query language are outside the scope of this tutorial, you can inspect the conents of the Python script, consult some of the examples in the query service repo, and use the live schema browser as guides for constructing more advanced queries.</p>"}]}